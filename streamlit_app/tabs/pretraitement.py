import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns 
from PIL import Image
from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV, learning_curve
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PolynomialFeatures
from st_aggrid import GridOptionsBuilder, AgGrid, GridUpdateMode, DataReturnMode

title = "Affichage 2D & Tests statistiques"
sidebar_name = title


def run():

    df = pd.read_csv("assets/bank.csv", sep=",", header=0)
    df_1 = df.copy()

    # Suppresion de 'duration' & 'campaign'
    df_1 = df_1.drop('duration', axis = 1)
    df_1 = df_1.drop('campaign', axis = 1)

# Suppresion des 2 lignes incoh√©rentes : 
    df_1 = df_1[(df_1['poutcome'] != 'unknown') | (df_1['previous'] == 0)]

    st.title(title)
    
    st.markdown("---")
    
    tab1, tab2 = st.tabs(["Affichage 2D", "Tests Statistiques"])
    
    with tab1:

        st.header('ACP : Analyse des composantes principales')
        st.write(''' Afin d'avoir une meilleure repr√©sentation du jeu de donn√©es, nous allons opter pour une analyse en composante principale, l'ACP permet de transformer des variables qui sont corr√©l√©es en variables d√©-corr√©l√©es. Cette analyse permet de r√©duire le nombre de variables du jeu de donn√©es pour simplifier les observations tout en conservant un maximum d'informations. 
    Pour cela, il est n√©cessaire de n‚Äôavoir que des variables num√©riques. Nous transformons donc les variables cat√©gorielles en variables indicatrices. Nos donn√©es ne sont pas sur la m√™me base, on met tous les champs √† la m√™me √©chelle en centrant et r√©duisant les donn√©es (MinMaxScaler)
    Cette analyse permet de r√©duire le nombre de variables du jeu de donn√©es pour simplifier les observations tout en conservant un maximum d'informations.''' )
        
        st.image(Image.open("assets/PCA_1.png"), use_column_width=False, caption='ACP',  width=400)
    
        st.write(''' En projetant les 2 premi√®res composantes, nous atteignons 24% d'inertie expliqu√©e. Les donn√©es sont tr√®s m√©lang√©es et il est vraiment difficile de distinguer des zones d√©partageant le deposit.
    On peut tout de m√™me voir que les points rouges sont majoritaires en bas √† gauche et les points bleus le sont en haut √† droite du graphique. ''')

        st.header('Autre m√©thode de r√©duction de dimension')
        st.write('ISOMAP & TNSE : ')
        st.image(Image.open("assets/PCA_2.png"), use_column_width=False, caption='ISOMAP & TNSP',   width=750)


        st.write(''' On a test√© deux autres m√©thodes : Isomap et TNSE . Nous parvenons au m√™me constat que pour le PCA. Il y a bien certaines zones regroupant uniquement ou majoritairement du rouge. 
    Mais on a √©galement √©norm√©ment de zones o√π les points rouges et bleus sont m√©lang√©s. 
    Ce que montre ces 3 graphiques, c‚Äôest qu‚Äôil sera tr√®s difficile de d√©partager l‚Äôensemble des donn√©es. Nous ne sommes pas sur un sujet simple et nous ne pourrons donc pas esp√©rer de r√©sultat avoisinant les 100%. ''')

    with tab2:

        st.header("Analyse des variables cat√©gorielles √† l'aide du test du ùúí2")
        st.write('Le tableau suivant nous donne les informations suivantes : statistique du test, p-value, \
    degr√© de libert√©, et V de Cramer (coefficient de corr√©lation du ùúí2)')

        from scipy.stats import chi2_contingency
        df_1_cat = df_1.select_dtypes('object')
        def V_Cramer(table, N):
            stat_chi2 = chi2_contingency(table)[0]
            k = table.shape[0]
            r = table.shape[1]
            phi_2 = max(0,(stat_chi2)/N - ((k - 1)*(r - 1)/(N - 1)))
            k_b = k - (np.square(k - 1) / (N - 1))
            r_b = r - (np.square(r - 1) / (N - 1))   
            return np.sqrt(phi_2 / min(k_b - 1, r_b - 1))

        dico = {}
        for col in df_1_cat.columns:
            table = pd.crosstab(df_1_cat[col], df_1['deposit'])
            res = chi2_contingency(table)
            dico[col] = [res[0], res[1], res[2], V_Cramer(table, df_1.shape[0])]
        
        
        stats = pd.DataFrame.from_dict(dico).transpose()
        stats = stats.rename(columns={0:'chi 2', 1:'p-value', 2:'DoF', 3:'V de Cramer'})
        st.dataframe(stats)
        st.write(''' les p-values sont toutes sous la barre des 5%, on rejette donc l'hypoth√®se nulle H0 'les 2 variables test√©es sont ind√©pendantes'.\n\
    De m√™me, le V de Cramer pr√©sente des valeurs insuffisantes notamment pour les variables 'marital', 'default' ''')
        
        st.header("Analyse des variables num√©riques √† l'aide du test ANOVA")

        st.write(''' Une ANOVA ('analyse de variance') est utilis√©e pour d√©terminer si oui ou non les moyennes de trois groupes ind√©pendants ou plus sont √©gales. \n\
    Une ANOVA utilise les hypoth√®ses nulles et alternatives suivantes :  \n\
    - H0 : Toutes les moyennes de groupe sont √©gales.  \n\
    - HA : Au moins une moyenne de groupe est diff√©rente des autres. ''')

        st.write(''' Comprendre la valeur P dans ANOVA :  \n Si cette valeur de p est inf√©rieure √† Œ± = 0,05, \n\
    nous rejetons l'hypoth√®se nulle de ANOVA et \n concluons qu'il existe une diff√©rence statistiquement \n\
    significative entre les moyennes des trois groupes. \n\
    Sinon, si la valeur de p n'est pas inf√©rieure √† Œ± = 0,05, nous ne rejetons pas l'hypoth√®se nulle \n\
    et concluons que nous n'avons pas suffisamment de preuves pour dire qu'il existe une diff√©rence statistiquement significative entre les moyennes des trois groupes. ''')
        
        data_anova = {'F':[13.587852,73.92161, 35.584154, 261.977062, 222.509262],
            'PR(>F)':[0.000229,9.212031e-18,2.516763e-09,2.925198e-58,7.740962e-50]}
        df_anova = pd.DataFrame(data_anova, index=['age','balance','days','pdays','previous'])
        st.dataframe(df_anova) 
        st.write(''' Les p-values sont toutes sous la barre des 5%, on rejette donc l'hypoth√®se nulle H0, de m√™me, le V de Cramer pr√©sente des valeurs insuffisantes la plupart des variables, on peut en conclure qu‚Äôil est difficile de mettre en √©vidence des features indispensables.''' )

        st.header(''' Correlation entre variables cat√©goriques : ''')
        # Cr√©ation d'un dataframe avec les variables cat√©goriques :
        df_1_cat = df_1.select_dtypes('object')
        df_1_cat.columns
        from itertools import product

        # Cr√©ation d'un dataframe avec les variables cat√©goriques :
        df_1_cat = df_1.select_dtypes('object')
        df_1_cat.columns
        # Split de df_1_cat en 2 parties : 
        df_1_cat_var1 = ('job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit')

        #  Chi-2 Test :
        ## Cr√©ation de toutes les combinaison Creating all possible combinations between the above two variables list
        cat_var_prod = list(product(df_1_cat_var1,df_1_cat_var1, repeat = 1))

        ## Cr√©ation d'une liste result et la remplir avec les p values issues du chi2 test : 
        import scipy.stats as ss

        result = []
        for i in cat_var_prod:
            if i[0] != i[1]:
                result.append((i[0],i[1],list(ss.chi2_contingency(pd.crosstab(df_1_cat[i[0]], df_1_cat[i[1]])))[1]))

        chi_test_output = pd.DataFrame(result, columns = ['var1', 'var2', 'coeff'])
        ## Using pivot function to convert the above DataFrame into a crosstab
        chi_test_output.pivot(index='var1', columns='var2', values='coeff')
        # Heatmap : 
        st.image(Image.open("assets/corr_cat.png"), use_column_width=False, caption='Heatmap Chi-2',  width=800)
        #fig = plt.figure(figsize=(15,8))
        #sns.heatmap(chi_test_output.pivot(index='var1', columns='var2', values='coeff'), annot=True, cmap='Blues')        
        #st.write(fig)
        st.write(''' Conclusions :\n Pour rappel : Chaque test statistique dispose de ce qu‚Äôon appelle la p-value.\n\ 
        On peut la voir comme une valeur r√©f√©rence pour d√©cider du rejet ou non de l‚Äôhypoth√®se nulle.\n\ 
        Si cette derni√®re est en-dessous de 5% alors on rejette l‚Äôhypoth√®se nulle (¬´ les deux variables test√©es sont ind√©pendantes ¬ª) \n\ ''')
        st.write(''' Ici p-value > 0,05 pour les variables suivantes: ''')
        st.write(''' - Education / default''')
        st.write(''' - Education / housing ''') 
        st.write(''' - Loan / contact''') 
        st.write(''' - Marital / default''')
        st.write('''Ces variables sont ind√©pendantes ''')
        
        st.header(''' Correlation entre variables num√©riques : ''')
        st.write(''' Le coefficient de Pearson est un indice refl√©tant une relation lin√©aire entre deux variables continues. Le coefficient de corr√©lation varie entre -1 et +1, 0 refl√©tant une relation nulle entre les deux variables, une valeur n√©gative (corr√©lation n√©gative) signifiant que lorsqu'une des variable augmente, l'autre diminue ; tandis qu'une valeur positive (corr√©lation positive) indique que les deux variables varient ensemble dans le m√™me sens.
    La valeur de r obtenue est une estimation de la corr√©lation entre deux variables continues dans la population. D√®s lors, sa valeur fluctuera d'un √©chantillon √† l'autre. On veut donc savoir si, dans la population ces deux variables sont r√©ellement corr√©l√©es ou pas. On doit donc r√©aliser un test d'hypoth√®se.

    - H0: Pas de corr√©lation entre les deux variables : œÅ = 0 
    - HA: Corr√©lation entre les deux variables : œÅ ‚â† 0

    Faisons apparaitre la heatmap des corr√©lations entre les valeurs num√©riques : ''')
        st.image(Image.open("assets/corr_num.png"), use_column_width=False, caption='Heatmap Pearson',  width=800)
        #fig = plt.figure(figsize=(16, 6))
        #sns.heatmap(df_1.corr(), vmin=-1, vmax=1, annot=True, cmap='Blues')
        #st.write(fig)

        st.header(''' Correlation entre valeurs cat√©goriques test de Kendall : ''')
        st.write('''le tau de Kendall est une statistique qui mesure l‚Äôassociation entre deux variables.
    Plus sp√©cifiquement, le tau de Kendall mesure la corr√©lation de rang entre deux variables.
    Le tau s‚Äôinterpr√™te de la m√™me fa√ßon que les autres coeffcients de corr√©lation. Sa valeur est comprise entre -1 et 1 : s‚Äôil s‚Äôapproche de 1, on peut supposer l‚Äôexistence d‚Äôune corr√©lation positive (variation dans le m√™me sens), 
    s‚Äôil tend vers -1, on peut dire qu‚Äôil existe une corr√©lation n√©gative et si le tau est proche de 0, il est fort probable qu‚Äôil n‚Äôy ait aucune liaison entre les 2 variables.  ''')

        from scipy.stats import kendalltau
        corr, _ = kendalltau(df_1['balance'], df_1['job'])
        st.write('- Coefficient de corr√©lation Kendall entre balance et job : %.5f' % corr)
        corr, _ = kendalltau(df_1['day'], df_1['month'])
        st.write(''' - Coefficient de corr√©lation Kendall entre day et month : %.5f''' % corr)
        st.write(''' - Grande probabilit√© qu'il n'y ait aucune liaison entre les variables ''')

