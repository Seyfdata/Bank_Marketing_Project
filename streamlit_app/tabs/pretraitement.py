import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns 
from PIL import Image
from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV, learning_curve
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PolynomialFeatures
from st_aggrid import GridOptionsBuilder, AgGrid, GridUpdateMode, DataReturnMode

title = "Affichage 2D & Tests statistiques"
sidebar_name = title


def run():

    df = pd.read_csv("assets/bank.csv", sep=",", header=0)
    df_1 = df.copy()

    # Suppresion de 'duration' & 'campaign'
    df_1 = df_1.drop('duration', axis = 1)
    df_1 = df_1.drop('campaign', axis = 1)

# Suppresion des 2 lignes incohÃ©rentes : 
    df_1 = df_1[(df_1['poutcome'] != 'unknown') | (df_1['previous'] == 0)]

    st.title(title)
    
    st.markdown("---")
    
    tab1, tab2 = st.tabs(["Affichage 2D", "Tests Statistiques"])
    
    with tab1:

        st.header('ACP : Analyse des composantes principales')
        st.write(''' Afin d'avoir une meilleure reprÃ©sentation du jeu de donnÃ©es, nous allons opter pour une analyse en composante principale, l'ACP permet de transformer des variables qui sont corrÃ©lÃ©es en variables dÃ©-corrÃ©lÃ©es. Cette analyse permet de rÃ©duire le nombre de variables du jeu de donnÃ©es pour simplifier les observations tout en conservant un maximum d'informations. 
    Pour cela, il est nÃ©cessaire de nâ€™avoir que des variables numÃ©riques. Nous transformons donc les variables catÃ©gorielles en variables indicatrices. Nos donnÃ©es ne sont pas sur la mÃªme base, on met tous les champs Ã  la mÃªme Ã©chelle en centrant et rÃ©duisant les donnÃ©es (MinMaxScaler)
    Cette analyse permet de rÃ©duire le nombre de variables du jeu de donnÃ©es pour simplifier les observations tout en conservant un maximum d'informations.''' )
        
        st.image(Image.open("assets/PCA_1.png"), use_column_width=False, caption='ACP',  width=400)
    
        st.write(''' En projetant les 2 premiÃ¨res composantes, nous atteignons 24% d'inertie expliquÃ©e. Les donnÃ©es sont trÃ¨s mÃ©langÃ©es et il est vraiment difficile de distinguer des zones dÃ©partageant le deposit.
    On peut tout de mÃªme voir que les points rouges sont majoritaires en bas Ã  gauche et les points bleus le sont en haut Ã  droite du graphique. ''')

        st.header('Autre mÃ©thode de rÃ©duction de dimension')
        st.write('ISOMAP & TNSE : ')
        st.image(Image.open("assets/PCA_2.png"), use_column_width=False, caption='ISOMAP & TNSP',   width=750)


        st.write(''' On a testÃ© deux autres mÃ©thodes : Isomap et TNSE . Nous parvenons au mÃªme constat que pour le PCA. Il y a bien certaines zones regroupant uniquement ou majoritairement du rouge. 
    Mais on a Ã©galement Ã©normÃ©ment de zones oÃ¹ les points rouges et bleus sont mÃ©langÃ©s. 
    Ce que montre ces 3 graphiques, câ€™est quâ€™il sera trÃ¨s difficile de dÃ©partager lâ€™ensemble des donnÃ©es. Nous ne sommes pas sur un sujet simple et nous ne pourrons donc pas espÃ©rer de rÃ©sultat avoisinant les 100%. ''')

    with tab2:

        st.header("Analyse des variables catÃ©gorielles Ã  l'aide du test du ğœ’2")
        st.write('Le tableau suivant nous donne les informations suivantes : statistique du test, p-value, \
    degrÃ© de libertÃ©, et V de Cramer (coefficient de corrÃ©lation du ğœ’2)')

        from scipy.stats import chi2_contingency
        df_1_cat = df_1.select_dtypes('object')
        def V_Cramer(table, N):
            stat_chi2 = chi2_contingency(table)[0]
            k = table.shape[0]
            r = table.shape[1]
            phi_2 = max(0,(stat_chi2)/N - ((k - 1)*(r - 1)/(N - 1)))
            k_b = k - (np.square(k - 1) / (N - 1))
            r_b = r - (np.square(r - 1) / (N - 1))   
            return np.sqrt(phi_2 / min(k_b - 1, r_b - 1))

        dico = {}
        for col in df_1_cat.columns:
            table = pd.crosstab(df_1_cat[col], df_1['deposit'])
            res = chi2_contingency(table)
            dico[col] = [res[0], res[1], res[2], V_Cramer(table, df_1.shape[0])]
        
        
        stats = pd.DataFrame.from_dict(dico).transpose()
        stats = stats.rename(columns={0:'chi 2', 1:'p-value', 2:'DoF', 3:'V de Cramer'})
        st.dataframe(stats)
        st.write(''' les p-values sont toutes sous la barre des 5%, on rejette donc l'hypothÃ¨se nulle H0 'les 2 variables testÃ©es sont indÃ©pendantes'.\n\
    De mÃªme, le V de Cramer prÃ©sente des valeurs insuffisantes notamment pour les variables 'marital', 'default' ''')
        
        st.header("Analyse des variables numÃ©riques Ã  l'aide du test ANOVA")

        st.write(''' Une ANOVA ('analyse de variance') est utilisÃ©e pour dÃ©terminer si oui ou non les moyennes de trois groupes indÃ©pendants ou plus sont Ã©gales. \n\
    Une ANOVA utilise les hypothÃ¨ses nulles et alternatives suivantes :  \n\
    - H0 : Toutes les moyennes de groupe sont Ã©gales.  \n\
    - HA : Au moins une moyenne de groupe est diffÃ©rente des autres. ''')

        st.write(''' Comprendre la valeur P dans ANOVA :  \n Si cette valeur de p est infÃ©rieure Ã  Î± = 0,05, \n\
    nous rejetons l'hypothÃ¨se nulle de ANOVA et \n concluons qu'il existe une diffÃ©rence statistiquement \n\
    significative entre les moyennes des trois groupes. \n\
    Sinon, si la valeur de p n'est pas infÃ©rieure Ã  Î± = 0,05, nous ne rejetons pas l'hypothÃ¨se nulle \n\
    et concluons que nous n'avons pas suffisamment de preuves pour dire qu'il existe une diffÃ©rence statistiquement significative entre les moyennes des trois groupes. ''')
        
        data_anova = {'F':[13.587852,73.92161, 35.584154, 261.977062, 222.509262],
            'PR(>F)':[0.000229,9.212031e-18,2.516763e-09,2.925198e-58,7.740962e-50]}
        df_anova = pd.DataFrame(data_anova, index=['age','balance','days','pdays','previous'])
        st.dataframe(df_anova) 
        st.write(''' Les p-values sont toutes sous la barre des 5%, on rejette donc l'hypothÃ¨se nulle H0, de mÃªme, le V de Cramer prÃ©sente des valeurs insuffisantes la plupart des variables, on peut en conclure quâ€™il est difficile de mettre en Ã©vidence des features indispensables.''' )

        st.header(''' Correlation entre variables catÃ©goriques : ''')
        # CrÃ©ation d'un dataframe avec les variables catÃ©goriques :
        df_1_cat = df_1.select_dtypes('object')
        df_1_cat.columns
        from itertools import product

        # CrÃ©ation d'un dataframe avec les variables catÃ©goriques :
        df_1_cat = df_1.select_dtypes('object')
        df_1_cat.columns
        # Split de df_1_cat en 2 parties : 
        df_1_cat_var1 = ('job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit')

        #  Chi-2 Test :
        ## CrÃ©ation de toutes les combinaison Creating all possible combinations between the above two variables list
        cat_var_prod = list(product(df_1_cat_var1,df_1_cat_var1, repeat = 1))

        ## CrÃ©ation d'une liste result et la remplir avec les p values issues du chi2 test : 
        import scipy.stats as ss

        result = []
        for i in cat_var_prod:
            if i[0] != i[1]:
                result.append((i[0],i[1],list(ss.chi2_contingency(pd.crosstab(df_1_cat[i[0]], df_1_cat[i[1]])))[1]))

        chi_test_output = pd.DataFrame(result, columns = ['var1', 'var2', 'coeff'])
        ## Using pivot function to convert the above DataFrame into a crosstab
        chi_test_output.pivot(index='var1', columns='var2', values='coeff')
        # Heatmap : 
        st.image(Image.open("assets/corr_cat.png"), use_column_width=False, caption='Heatmap Chi-2',  width=800)
        #fig = plt.figure(figsize=(15,8))
        #sns.heatmap(chi_test_output.pivot(index='var1', columns='var2', values='coeff'), annot=True, cmap='Blues')        
        #st.write(fig)
        st.write(''' Conclusions :\n Pour rappel : Chaque test statistique dispose de ce quâ€™on appelle la p-value.\n\ 
        On peut la voir comme une valeur rÃ©fÃ©rence pour dÃ©cider du rejet ou non de lâ€™hypothÃ¨se nulle.\n\ 
        Si cette derniÃ¨re est en-dessous de 5% alors on rejette lâ€™hypothÃ¨se nulle (Â« les deux variables testÃ©es sont indÃ©pendantes Â») \n\ ''')
        st.write(''' Ici p-value > 0,05 pour les variables suivantes: ''')
        st.write(''' - Education / default''')
        st.write(''' - Education / housing ''') 
        st.write(''' - Loan / contact''') 
        st.write(''' - Marital / default''')
        st.write('''Ces variables sont indÃ©pendantes ''')
        
        st.header(''' Correlation entre variables numÃ©riques : ''')
        st.write(''' Le coefficient de Pearson est un indice reflÃ©tant une relation linÃ©aire entre deux variables continues. Le coefficient de corrÃ©lation varie entre -1 et +1, 0 reflÃ©tant une relation nulle entre les deux variables, une valeur nÃ©gative (corrÃ©lation nÃ©gative) signifiant que lorsqu'une des variable augmente, l'autre diminue ; tandis qu'une valeur positive (corrÃ©lation positive) indique que les deux variables varient ensemble dans le mÃªme sens.
    La valeur de r obtenue est une estimation de la corrÃ©lation entre deux variables continues dans la population. DÃ¨s lors, sa valeur fluctuera d'un Ã©chantillon Ã  l'autre. On veut donc savoir si, dans la population ces deux variables sont rÃ©ellement corrÃ©lÃ©es ou pas. On doit donc rÃ©aliser un test d'hypothÃ¨se.

    - H0: Pas de corrÃ©lation entre les deux variables : Ï = 0 
    - HA: CorrÃ©lation entre les deux variables : Ï â‰  0

    Faisons apparaitre la heatmap des corrÃ©lations entre les valeurs numÃ©riques : ''')
        st.image(Image.open("assets/corr_num.png"), use_column_width=False, caption='Heatmap Pearson',  width=800)
        #fig = plt.figure(figsize=(16, 6))
        #sns.heatmap(df_1.corr(), vmin=-1, vmax=1, annot=True, cmap='Blues')
        #st.write(fig)

        st.header(''' Correlation entre valeurs catÃ©goriques test de Kendall : ''')
        st.write('''le tau de Kendall est une statistique qui mesure lâ€™association entre deux variables.
    Plus spÃ©cifiquement, le tau de Kendall mesure la corrÃ©lation de rang entre deux variables.
    Le tau sâ€™interprÃªte de la mÃªme faÃ§on que les autres coeffcients de corrÃ©lation. Sa valeur est comprise entre -1 et 1 : sâ€™il sâ€™approche de 1, on peut supposer lâ€™existence dâ€™une corrÃ©lation positive (variation dans le mÃªme sens), 
    sâ€™il tend vers -1, on peut dire quâ€™il existe une corrÃ©lation nÃ©gative et si le tau est proche de 0, il est fort probable quâ€™il nâ€™y ait aucune liaison entre les 2 variables.  ''')

        from scipy.stats import kendalltau
        corr, _ = kendalltau(df_1['balance'], df_1['job'])
        st.write('- Coefficient de corrÃ©lation Kendall entre balance et job : %.5f' % corr)
        corr, _ = kendalltau(df_1['day'], df_1['month'])
        st.write(''' - Coefficient de corrÃ©lation Kendall entre day et month : %.5f''' % corr)
        st.write(''' - Grande probabilitÃ© qu'il n'y ait aucune liaison entre les variables ''')

